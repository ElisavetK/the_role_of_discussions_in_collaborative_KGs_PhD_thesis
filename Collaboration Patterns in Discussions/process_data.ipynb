{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate threads and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import json\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "#this is to extent the size of the reading csv cell\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "file_path='path_to_raw+data'\n",
    "save_path_trash='path_to_save_data'\n",
    "\n",
    "\n",
    "def UTC_separation(thread):\n",
    "    posts_lst=[]\n",
    "    posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',thread)# split the posts based on the timestamp\n",
    "    for i in range(0,len(posts)-1, 2):\n",
    "        posts_lst.append(posts[i]+posts[i+1])# connects post with timestamp because the splot separates them\n",
    "    \n",
    "    return posts_lst\n",
    "\n",
    "#to separate discussions with no title\n",
    "def thread_without_title(temp, page_title, save_path_data):\n",
    "    if ((temp[0:2]=='{{') and (('documentation' in temp) or ('Documentation' in temp))) or ((temp[0:2]=='{{') and (('Interwiki conflict' in temp) or ('interwiki conflict' in temp))) or ('#REDIRECT' in temp): # checks if the text includes only documentation, not discussions\n",
    "         trash=pd.DataFrame({'page_title':[page_title], 'text':temp})\n",
    "         trash.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))\n",
    "    else:\n",
    "        thread_title='No subject'\n",
    "        posts_lst = UTC_separation(temp)\n",
    "        #store posts with the corresponding sibject\n",
    "        df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "        #save to csv \n",
    "        df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "\n",
    "\n",
    "def thread_with_title(titles, temp, page_title, save_path_data):\n",
    "    for j in range(len(temp)):\n",
    "        if (temp[j] in titles):\n",
    "            if (j+1)==(len(temp)):\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = []\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "            else:\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = UTC_separation(temp[j+1])\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "        \n",
    "    \n",
    "\n",
    "#function to create a file with the separated posts  \n",
    "def separate_discussions(text, page_title, save_path_data):\n",
    "\n",
    "    titles = re.findall('==(.*)==', text)# find the titles in the discussion  \n",
    "    titles=[s.replace('=', '') for s in titles]# removes unecessery = in the title\n",
    "    \n",
    "    \n",
    "    #titles=list(set(titles)) \n",
    "    temp= text.split('==') # split the text based on == , includided in the title\n",
    "    temp = [x for x in temp if x != ''] # stores the splited parts\n",
    "    temp=[s.strip('=') for s in temp]# removes unecessery = in the title\n",
    "\n",
    "    # ---Step 1: there is no title in the page\n",
    "    if titles==[]: # check if there is title in the begining of the page\n",
    "        thread_without_title(temp[0], page_title, save_path_data) \n",
    "    else:\n",
    "        if temp[0] not in titles:\n",
    "            thread_without_title(temp[0], page_title, save_path_data)# ---Step 2: the first thread do not have title \n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ignores the first if does not have a title and process the rest threads\n",
    "        else:\n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ---Step 3: a;; threads have titles \n",
    "\n",
    "\n",
    "\n",
    "file_lst=['items.csv','properties.csv']\n",
    "    \n",
    "\n",
    "for file in file_lst:\n",
    "    save_path_data='path'+str(file) # path to save posts and threads\n",
    "    data=pd.read_csv(str(file_path)+str(file), encoding='utf-8')# read file\n",
    "    for row in range(len(data)):# read row by row (i.e., page by page)\n",
    "        print(row)\n",
    "        # print(data.loc[row,'page_title'])\n",
    "        # print(data.loc[row,'text'])\n",
    "        page_title=data.loc[row,'page_title']\n",
    "        if isinstance(data.loc[row,'text'],str):# check of the row is nan. If it is nan the type is float not str\n",
    "            separate_discussions(data.loc[row,'text'], page_title, save_path_data)\n",
    "        else:\n",
    "            empty=pd.DataFrame({'page_title':[page_title], 'text':'empty'})\n",
    "            empty.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract username and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# find usernmae based on [[User talk:]] pattern\n",
    "def find_username_User_talk(text):\n",
    "    pattern=re.compile(r\"(\\[)?(User:|User talk:|user:|user talk:|User Talk:|User_talk:|User_Talk:|user_talk:|Utente:|Usuário:|Utilisateur:|Utilizator:|Usuari Discussió:|Usuario discusión:|Usuario:|Usuario Discusi\\u00f3n:|kullanıcı:|Kullan\\u0131c\\u0131:|:USER TALK:|:USER:|U:|u:|Benutzer:|Benutzer Diskussion:|Special:Contributions/|welcominguser=|Discussão:|Dyskusja_Wikipedysty:|사용자토론:)(.*?)(\\||/|\\])\") # the regular expression I need to extract \n",
    "    usernames=pattern.finditer(text)\n",
    "    #search for the username\n",
    "    names=[]        \n",
    "    for match in usernames:\n",
    "        #print(match.group(2))\n",
    "        names.append([match.group(3),match.end()])\n",
    "        \n",
    "    if len(names)>0:\n",
    "        max_loc=max([sublist[1] for sublist in names])\n",
    "        name=[sublist[0] for sublist in names if sublist[1]==max_loc]\n",
    "    else: name=[]\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#funstion to extract username from the post\n",
    "def extract_info(raw_text, anon):\n",
    "    timestamp='No date'\n",
    "    username= 'Unsigned post'\n",
    "    text1=raw_text.replace('‎','')\n",
    "    t_split=text1.split('(UTC)') # this is mainly for property proposal, because after (UTC) it includes remaining punctuation\n",
    "    t_connect1=[str(part)+'(UTC)' for part in t_split[ :-1]] # remove the final part with punctuaton\n",
    "    t_connect='(UTC)'.join(t_connect1) # connect again the text\n",
    "    \n",
    "    if t_connect.endswith('(UTC)'):# if UTC in not in the text, the post does not have a signature or timestmap so the usernmae will be assigned as unsigned and the timestamp with no date\n",
    "        peaces=re.split(\"(\\d+:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\", t_connect)\n",
    "        if len(peaces)>1:\n",
    "            timestamp=peaces[1]\n",
    "            u_user_talk=find_username_User_talk(peaces[0])# step 1--- detect the pattern [[user talk:]] to find the username\n",
    "            \n",
    "            if len(u_user_talk)<1:# if step 1 do not work\n",
    "                u_from_list= [name for name in username_lst if name in peaces[0]]# step 2 ----- check the usernmae list\n",
    "                \n",
    "                for name in u_from_list:\n",
    "                    text=peaces[0].replace('(talk) ','')\n",
    "                    u_peaces=re.split(name,text)\n",
    "                    if len(u_peaces[-1])<10:\n",
    "                        print(name)\n",
    "                        return timestamp, name, anon\n",
    "                if len(u_from_list)<1 or username== 'Unsigned post':# if step 1 and 2 do not work the username will be assigned as anonymous\n",
    "                    anon += 1\n",
    "                    username= 'Anonymous_username_'+str(anon)\n",
    "                    print(username)\n",
    "                    return timestamp, username, anon\n",
    "                    \n",
    "            else:\n",
    "                print(u_user_talk)\n",
    "                return timestamp, u_user_talk[0], anon\n",
    "        else:\n",
    "            print(username)\n",
    "            return timestamp, username, anon    \n",
    "    else:\n",
    "        print(username)\n",
    "        return timestamp, username, anon\n",
    "\n",
    "    \n",
    "\n",
    " #---------RUN-----------\n",
    "\n",
    "\n",
    "file_path='path_to_directory__files_for_process'\n",
    "save_path='path_to_directory_to_save_files'\n",
    "\n",
    "# load the list with usernames we have gathered during the dump process \n",
    "# read usernames \n",
    "my_file = open(\"path_to_username_list\", \"r\", encoding='utf-8')\n",
    "data = my_file.read()\n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "username_lst = data.split(\"\\n\")\n",
    "my_file.close()\n",
    "# #separate usernmaes based on length because there are usernmaes with one character and confuse the correct detection\n",
    "# usernames_short=[n for n in username_lst if len(n)<=2]\n",
    "# usernames_long=[n for n in username_lst if len(n)>3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "anon=0 # this is to give names in case we can not recognise a username\n",
    "for dump in os.listdir(str(file_path)):\n",
    "    # dump='property_proposal_all.csv'\n",
    "    df=pd.read_csv(str(file_path)+str(dump), encoding='utf-8')\n",
    "    for row in range(len(df)):\n",
    "        print(dump, row)\n",
    "        # test=\"=== {{TranslateThis | anchor = en| en = Kinsky–Halm Catalogue <!-- |xx = property names in some other languages -->}} ==={{Property proposal|status\t\t\t= 7161|description\t\t= {{TranslateThis | en = Number in the Kinsky–Halm Catalogue of the works of Ludwig van Beethoven, which do not have opus numbers, or are fragmentary.<!-- | xx = descriptions in other languages --> }}|subject item           = {{Q|616043}}|infobox parameter\t= [[en:catalogue:infobox musical composition]]<!-- Wikipedia infobox parameters, if any; ex: \"population\" in [[:en:template:infobox settlement]] -->|datatype\t\t= external-id<!-- put datatype here (one of: item, external-id, string, media, coordinates, monolingual text, multilingual text, time, URL, number, geo-shape, tabular, lexeme, form, sense, mathematical expression, or a possibly new one) -->|domain\t\t\t= <!-- entity type (item, property, lexeme, form, sense) and types of items that may bear this property; give QIDs if possible -->|allowed values\t\t= \\d+<!-- type of linked items (Q template or text), string pattern (regex if possible), list or range of allowed values, etc. -->|allowed units          = <!-- units that are allowed for values of this property -->|source\t\t\t= https://en.wikipedia.org/wiki/WoO <!-- external reference URL, Wikipedia list article, etc. --><!-- you should provide 3 examples at least-->|example 1\t\t= {{Q|8076022}} → 123 <!-- {{Q|1}} → value -->|example 2\t\t= {{Q|166550}} → 4|example 3\t\t= {{Q|166173}} → 6|example 4\t\t= <!--  if convenient, include more examples by following the same pattern: |example ...\t\t=  {{Q|1}} → value-->|planned use            = Add to Authority control<!-- what use do you plan to make of the property within the next month -->|number of ids          = <!-- for identifiers, the number of ids available from the source -->|expected completeness  = Q21873974 <!-- for identifiers, the coverage we can hope for in Wikidata: Q21873886 (always incomplete) or Q21873974 (eventually complete) -->|formatter URL\t\t= <!-- for identifiers, URL pattern where $​1 replaces the value -->|external links         = <!-- for identifiers, search string to pass to sister projects' Special:LinkSearch pages, e.g. example.com  -->|see also               = <!-- other, related properties -->|filter\t\t\t= <!-- sample: 7 digit number can be validated with edit filter [[Special:AbuseFilter/17]] -->|robot and gadget jobs\t= <!-- Should or are bots or gadgets doing any task with this? (Checking other properties for consistency, collecting data, etc.) -->|subpage\t\t= Werke ohne Opuszahl|topic\t\t\t= authority control}}====Motivation====This is an important catalogue, which we should be able to reference, possibly as authority control.  I leave it to more experienced folk to consider whether there should be a generic WoO and a separate Kinsky–Halm Catalogue property. (E.G. http://www.raff.org/music/catalog/opus/woo.htm is a WoO catalogue for Joachim Raff.)All&nbsp;the&nbsp;best: ''[[Q23041480|Rich]]&nbsp;[[User talk:Rich Farmbrough|Farmbrough]]'',&nbsp;<small>12:41,&nbsp;4&nbsp;August&nbsp;2019&nbsp;(UTC).</small><br />====\"\n",
    "        # timestamp_row, username_row, anon = extract_info(test, anon)\n",
    "        timestamp_row, username_row, anon = extract_info(df.loc[row,'post'], anon)\n",
    "        save_df=pd.DataFrame({'post':[df.loc[row,'post']],'page_title':[df.loc[row,'page_title']],'thread_title':[df.loc[row,'thread_title']],'urername':[username_row],'timestamp':[timestamp_row]})\n",
    "        save_df.to_csv(str(save_path)+str(dump),mode='a',index=False, header=not os.path.exists(str(save_path)+str(dump)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asign IDs to usernames, posts, threads, and pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usernames\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('path_to_file_with_username_inflo', encoding='utf-8')\n",
    "df_id=pd.DataFrame(columns=['UID','username','user_type','registration','rights','asking','responding','num_posts', 'talking_type'])\n",
    "for row in range(len(df)):\n",
    "    df_id.loc[row,'UID']='U'+str(row)\n",
    "    df_id.iloc[row,1:2]=df.iloc[row,2:3]\n",
    "    df_id.iloc[row,3:]=df.iloc[row,5:]\n",
    "df_id.to_csv(\"path_to_save\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post, thread, and page ids\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "UID_df=pd.read_csv(\"username_ids.csv\",encoding='utf-8')\n",
    "\n",
    "path='path_to_directory_with_post_info'\n",
    "\n",
    "p_id=0\n",
    "pt_id=0\n",
    "t_id=0\n",
    "for file in os.listdir(str(path)):\n",
    "\n",
    "    # file='items.csv'\n",
    "    df=pd.read_csv(str(path)+str(file),encoding='utf-8')\n",
    "\n",
    "    # dropping the rows having NaN values\n",
    "    df = df.dropna()\n",
    "    # To reset the indices\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df_id=pd.DataFrame(columns=['PID','post','PTID','page_title','TID','thread_title','UID','username','timestamp','answer2post'])\n",
    "\n",
    "    # set page_title ids\n",
    "    pt_uniqies=df['page_title'].unique()\n",
    "    pt_ids_df=None\n",
    "    pt_ids_df=pd.DataFrame({'title':pt_uniqies,'id':['PT'+str(x) for x in range(pt_id,pt_id+len(pt_uniqies))]})\n",
    "    pt_id=pt_id+len(pt_uniqies)\n",
    "\n",
    "    #set thread_title ids\n",
    "    t_ids_df=pd.DataFrame()\n",
    "    for pt in pt_uniqies:\n",
    "        df_pt=df.loc[df['page_title']==pt]\n",
    "        t_uniqies=df_pt['thread_title'].unique()\n",
    "        t_ids_df1=None\n",
    "        t_ids_df1=pd.DataFrame({'title':t_uniqies,'id':['T'+str(x) for x in range(t_id,t_id+len(t_uniqies))], 'page':[pt]*len(t_uniqies)})\n",
    "        t_id=t_id+len(t_uniqies)\n",
    "        t_ids_df=pd.concat([t_ids_df,t_ids_df1])\n",
    "\n",
    "    for row in range(len(df)):\n",
    "        print(file, row)\n",
    "        df_id.loc[row,'PID']='P'+str(p_id+row)\n",
    "        df_id.loc[row,'post']=df.loc[row,'post']\n",
    "        df_id.loc[row,'PTID']=pt_ids_df.loc[pt_ids_df['title']==df.loc[row,'page_title'],'id'].values[0]\n",
    "        df_id.loc[row,'page_title']=df.loc[row,'page_title']\n",
    "        df_id.loc[row,'TID']=t_ids_df.loc[((t_ids_df['title']==df.loc[row,'thread_title'])&(t_ids_df['page']==df.loc[row,'page_title'])),'id'].values[0]\n",
    "        df_id.loc[row,'thread_title']=df.loc[row,'thread_title']\n",
    "        df_id.loc[row,'UID']=UID_df.loc[UID_df['username']==df.loc[row,'urername'],'UID'].values[0]\n",
    "        df_id.loc[row,'username']=df.loc[row,'urername']\n",
    "        df_id.loc[row,'timestamp']=df.loc[row,'timestamp']\n",
    "        df_id.loc[row,'answer2post']=df.loc[row,'answer2post']\n",
    "        \n",
    "\n",
    "\n",
    "        # df_id.iloc[row,1:2]=df.iloc[row,2:3]\n",
    "        # df_id.iloc[row,3:]=df.iloc[row,5:]\n",
    "\n",
    "    p_id=p_id+row+1\n",
    "    df_id.to_csv('path_to_save_directory'+str(file), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration date\n",
    "\n",
    "We used a Wikidata API https://www.wikidata.org/w/api.php?action=help&modules=query%2Busers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the info\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://www.wikidata.org/w/api.php\"#info about the features you can extract https://www.wikidata.org/w/api.php?action=help&modules=query%2Busers\n",
    "\n",
    "\n",
    "editors_count=0\n",
    "line_count=0\n",
    "\n",
    "\n",
    "users = pd.DataFrame(columns=['userid','name','editcount', 'registration','rights','name_post'])#create a data frame with the features you choose\n",
    "\n",
    "editors_count=0\n",
    "#path to editors names\n",
    "with open('known_names_APIinput.txt', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        \n",
    "        #editors_in_each_line_count=0\n",
    "        for part in l.split(\"|\"):\n",
    "            editors_count += 1\n",
    "            print('count: ',editors_count, \" editor: \", part)\n",
    "            # line_count+=1\n",
    "            #editors_in_each_line_count+=1\n",
    "            \n",
    "            PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"users\",\n",
    "            \"ususers\": part,\n",
    "            # \"usprop\": \"blockinfo|editcount|registration|rights\"} #you choose features to extarct\n",
    "            \"usprop\": \"editcount|registration|rights\"} #you choose features to extarct\n",
    "            \n",
    "\n",
    "            \n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "            DATA = R.json()\n",
    "            \n",
    "            USERS = DATA[\"query\"][\"users\"]\n",
    "            # print(USERS[0])\n",
    "            row=USERS[0]\n",
    "            row['name_post']=part  \n",
    "            users = users.append(row, ignore_index=True)    \n",
    "            #unknown_users=pd.DataFrame(columns=['names'])  \n",
    "            # for u in USERS:\n",
    "            #     editors_count+=1\n",
    "            #     #if ('invalid' not in u.keys()) and ('missing' not in u.keys()):\n",
    "            #         #print(str(u[\"name\"]) + \" has \" + str(u[\"editcount\"]) + \" edits.\")\n",
    "            #         #print(str(u[\"name\"]) + ' with id ' + str(u['userid']))\n",
    "            #         #print(u['groupmemberships'])\n",
    "            #     users = users.append(u, ignore_index=True)\n",
    "            #     #else:unknown_users=unknown_users.append(u, ignore_index=True)\n",
    "                \n",
    "            \n",
    "            #path to save       \n",
    "            users.to_csv('known_names_info_output_v2.csv', encoding='utf-8', index=False)\n",
    "            #unknown_users.to_csv('/mnt/data/elisavetk/Theme_2/Post_Graph/unknown_editor_info.csv', encoding='utf-8', index=False,  mode='a', header=False)\n",
    "\n",
    "#print(editors_count)\n",
    "\n",
    "'''if editors_in_each_line_count == 49:\n",
    "    break'''\n",
    "#break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors' type \n",
    "\n",
    "registered, unregistered, unknown names, bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#load usernames found in discussions\n",
    "df=pd.read_csv('unique_usernames_found_in_discussions.csv', encoding='utf-8')\n",
    "u_df_lst=df['username'].tolist()\n",
    "\n",
    "# store known usernames. it is the merge of the above two\n",
    "u_common=df\n",
    "\n",
    "# store anonymou_usernames\n",
    "u_anonymous=[name for name in u_df_lst if 'Anonymous_username_'in str(name)]\n",
    "\n",
    "# store Unregistered users\n",
    "pattern1=r\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "pattern2=r\"(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "u_IP4=[name for name in u_df_lst if bool(re.match(pattern1,str(name)))]\n",
    "u_IP6=[name for name in u_df_lst if bool(re.match(pattern2,str(name)))]\n",
    "\n",
    "\n",
    "# len(u_common)+len(u_anonymous)+len(u_IP4)+len(u_IP6)\n",
    "known_names=u_common['username'].to_list()+u_anonymous+u_IP4+u_IP6 # registered and unregistered editors\n",
    "unknown_names=[name for name in u_df_lst if str(name) not in known_names] # unknwon names\n",
    "\n",
    "\n",
    "\n",
    "#check how many names are unknown names\n",
    "len(unknown_names)*100/len(u_df_lst)\n",
    "# the uknown names are 5% of the total names I found in discussions\n",
    "\n",
    "# create df with the username info for the three files\n",
    "u_data1_df=data(known_names)\n",
    "# u_data2_df=data(u_data2)\n",
    "u_data3_df=data(unknown_names)\n",
    "\n",
    "# store data anonymou_usernames\n",
    "u_anonymous=[name for name in df['username'] if 'Anonymous_username_'in str(name)]\n",
    "u_anonymous_data={name:[None,name,'anonymous_username',None, None, None] for name in u_anonymous}\n",
    "u_anonymous_df = pd.DataFrame([v for k, v in u_anonymous_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "\n",
    "# store data Unregistered users\n",
    "pattern1=r\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "pattern2=r\"(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "u_IP4=[name for name in df['username'] if bool(re.match(pattern1,str(name)))]\n",
    "u_IP4_data={name:[None,name,'unregistered',None, None, None] for name in u_IP4}\n",
    "u_IP4_df = pd.DataFrame([v for k, v in u_IP4_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "u_IP6=[name for name in df['username'] if bool(re.match(pattern2,str(name)))]\n",
    "u_IP6_data={name:[None,name,'unregistered',None, None, None] for name in u_IP6}\n",
    "u_IP6_df = pd.DataFrame([v for k, v in u_IP6_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "\n",
    "# df_full=pd.concat([u_data1_df,u_data2_df,u_data3_df,u_anonymous_df,u_IP4_df,u_IP6_df])\n",
    "df_full=pd.concat([u_data1_df,u_data3_df,u_anonymous_df,u_IP4_df,u_IP6_df])\n",
    "df_full.to_csv('username_type.csv',encoding='utf-8',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"username_type.csv\", encoding='utf-8')\n",
    "df_bots=pd.read_csv('bot_names.csv', encoding='utf-8')\n",
    "bot_name=df_bots['Name'].to_list()\n",
    "\n",
    "for i in range(len(df['username_type'])):\n",
    "     if df.loc[i,'name'] in bot_name:\n",
    "          df.loc[i,'username_type']='bot'\n",
    "\n",
    "df.to_csv('username_type.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors' edit history\n",
    "\n",
    "Process edit history to measure account age (the difference between registare date and last edit), max gaps of inacitvity (last edit and today), and number of edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "directory_path='username_edits/'# these are files with (username, timestamp) for editors' edits\n",
    "\n",
    "# last timestamp for the archived data\n",
    "today=pd.to_datetime('2023-03-02T00:00:00Z', format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "\n",
    "#load username information\n",
    "all_users=pd.read_csv('username_info_v2.csv',encoding='utf-8')\n",
    "#keep registered users\n",
    "#create a lsit with registered editors\n",
    "registered=all_users.loc[all_users['username_type'] == 'registered']\n",
    "# registered=registered1.head(1000)\n",
    "\n",
    "#lists to store data\n",
    "last_edit=[]\n",
    "acount_age=[]\n",
    "max_gap=[]\n",
    "last_today=[]\n",
    "number_of_edits=[]\n",
    "false_edits_names=[]\n",
    "mean_gap=[]\n",
    "median_gap=[]\n",
    "\n",
    "#for every registered user find last edit timestamp, acount age, max inactive gap, number of edits timestamps\n",
    "num=0\n",
    "for name in registered['name']:\n",
    "    num +=1\n",
    "    print(num, name)\n",
    "    # name='Ilmari Karonen'\n",
    "    # convert registration date to datetime type\n",
    "    registrasion_date=pd.to_datetime(registered.loc[registered['name']==name,'registration'], format='%Y-%m-%dT%H:%M:%SZ').to_frame()\n",
    "    # print(registrasion_date.iloc[0,0])\n",
    "\n",
    "    #load file with edit timestamps\n",
    "    #create a list with the files of users in the username_edits directory\n",
    "    onlyfiles = [f[:-4] for f in listdir('username_edits/') if isfile(join('/mnt/data/elisavetk/Wikidata_create_dataset/Create_csv_data_files/username_edits/', f))]\n",
    "\n",
    "    #check if the registered user has edits\n",
    "    if name in onlyfiles:\n",
    "        f=open(str(directory_path)+str(name)+'.txt','r')\n",
    "        data=f.readlines()\n",
    "\n",
    "        data_df=pd.DataFrame({'timestamp':data})#convert the list of timestamps to dataframe\n",
    "        dates=pd.to_datetime(data_df['timestamp'], format='%Y-%m-%dT%H:%M:%SZ\\n').to_frame()#convert the strings to date time\n",
    "\n",
    "        # check if there are edits with timestamp before 10/2012 and if yes, remove the timestamps and store the editors' names\n",
    "        false_edits=dates[dates['timestamp']<'2012-09-30T00:00:00']\n",
    "        if len(false_edits)>0:\n",
    "            false_edits_names.append(name)\n",
    "            # #find which rows have true, meaning which rows we want to extract\n",
    "            # num_rows_lst=dates.index[dates['timestamp']<'2012-09-30T00:00:00'].tolist()\n",
    "            #extract the rows from the edit hisotry data\n",
    "            dates=dates.loc[~dates.index.isin(false_edits.index)]\n",
    "\n",
    "\n",
    "        dates.sort_values(by='timestamp', ascending = True, inplace = True)# ascending order to date time\n",
    "            \n",
    "        #check if the editor has one or more edits\n",
    "        if len(dates)>1: \n",
    "            date_diff=dates['timestamp'].diff()/ np.timedelta64(1, 'M')# find the time intervals in months between edits\n",
    "            \n",
    "            # store the data\n",
    "            last_edit.append(dates.iloc[-1].values[0])\n",
    "            age=(dates.iloc[-1]-registrasion_date.iloc[0,0])/ np.timedelta64(1, 'M')\n",
    "            acount_age.append(age.values[0])\n",
    "            max_gap.append(np.nanmax(date_diff.to_frame()))\n",
    "            mean_gap.append(date_diff.to_frame().mean().values[0])\n",
    "            median_gap.append(date_diff.to_frame().median().values[0])\n",
    "            today_diff=(today-dates.iloc[-1])/ np.timedelta64(1, 'M')\n",
    "            last_today.append(today_diff.values[0])\n",
    "            number_of_edits.append(len(dates))\n",
    "        else:\n",
    "            #store the data\n",
    "            last_edit.append(dates.iloc[-1].values[0])\n",
    "            age=(dates.iloc[-1]-registrasion_date.iloc[0,0])/ np.timedelta64(1, 'M')\n",
    "            acount_age.append(age.values[0])\n",
    "            max_gap.append(0)\n",
    "            mean_gap.append(0)\n",
    "            median_gap.append(0)\n",
    "            today_diff=(today-dates.iloc[-1])/ np.timedelta64(1, 'M')\n",
    "            last_today.append(today_diff.values[0])\n",
    "            number_of_edits.append(len(dates))\n",
    "    else:\n",
    "        last_edit.append(0)\n",
    "        acount_age.append(0)\n",
    "        max_gap.append(0)\n",
    "        mean_gap.append(0)\n",
    "        median_gap.append(0)\n",
    "        today_diff=0\n",
    "        last_today.append(0)\n",
    "        number_of_edits.append(0)\n",
    "\n",
    "\n",
    "#save the full data\n",
    "my_data=pd.DataFrame({'last_edit':last_edit,'acount_age':acount_age,'max_gap':max_gap, 'mean_gap':mean_gap,'median_gap':median_gap,'last_today':last_today,'number_of_edits':number_of_edits})\n",
    "save_data=pd.concat([registered,my_data.set_index(registered.index)], axis=1)\n",
    "save_data.to_csv('username_info.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "#save the false edits names\n",
    "my_data_names=pd.DataFrame({'names':false_edits_names})\n",
    "my_data_names.to_csv('false_edits.csv', encoding='utf-8', index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status\n",
    "\n",
    "We measured above the difference between the last edit and today for each editor. We set a thresfold at 23 months for active and inactive editors. This threshold was measured using the percentile of max gaps of inactivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure percentile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('username_stats.csv') # this includes the max_gap\n",
    "\n",
    "status_threashold=np.percentile(df['max_gap'], 85)\n",
    "status_threashold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('username_stats.csv')\n",
    "\n",
    "status_threashold=23\n",
    "\n",
    "status=[]\n",
    "for last, num in zip(df.last_today, df.number_of_edits):\n",
    "    if last<status_threashold and num!=0:\n",
    "        status.append(1)# this is for active editors\n",
    "    else:status.append(0)# this is for inactive editors\n",
    "\n",
    "save_df=pd.concat([df,pd.DataFrame({'status':status})],axis=1)\n",
    "save_df.to_csv('username_status.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Level\n",
    "\n",
    "We extracted and saved the usernames of the different access levels from the Wikidata interface https://www.wikidata.org/wiki/Special:ListUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='files_access_level_usernames/'\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file[0]!='.':\n",
    "        print(file)\n",
    "        df_file=pd.read_csv(str(path)+str(file), encoding='utf-8')\n",
    "        names_list=df_file['clean_name'].to_list()\n",
    "        for name in df['name']:\n",
    "            if name in names_list:\n",
    "                label=df.loc[df['name']==name,'rights_label'].to_list()\n",
    "                if 'editor' in label: label.remove('editor')\n",
    "                label.append(file[:-4])\n",
    "                # label=str(label)+';'+str(file[:-4])\n",
    "                df.loc[df['name']==name,'rights_label']=';'.join(label)\n",
    "\n",
    "df.to_csv('registered_editors_access_levels.csv', index=None,encoding='utf-8')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1=editor\n",
    "14=acount creator\n",
    "15=bot \n",
    "4=buraucrats\n",
    "5=checkusers\n",
    "6=confirmed users\n",
    "7=flooders\n",
    "8= interface administartors\n",
    "9= IP exeptions\n",
    "10=rollblockers\n",
    "11=suppresors\n",
    "12=translator administrators\n",
    "13= wikidata staff\n",
    "2= property creator\n",
    "3= administrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('registered_editors_access_levels.csv', encoding='utf-8')\n",
    "\n",
    "data_lst=[]\n",
    "for i in range(len(df)):\n",
    "    labels=df.loc[i,'rights_label']\n",
    "    all_labels=labels.split(';')\n",
    "    for j in all_labels:\n",
    "        if j == 'editor':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],1))\n",
    "        elif j == 'acount_creator':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],14))\n",
    "        elif j == 'administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],3))\n",
    "        elif j == 'bots':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],15))\n",
    "        elif j == 'buraucrats':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],4))\n",
    "        elif j == 'checkusers':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],5))\n",
    "        elif j == 'confirmed_users':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],6))\n",
    "        elif j == 'flooders':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],7))\n",
    "        elif j == 'interface_administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],8))\n",
    "        elif j == 'IP_exeptions':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],9))\n",
    "        elif j == 'property_creator':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],2))\n",
    "        elif j == 'rollblockers':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],10))\n",
    "        elif j == 'suppresors':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],11))\n",
    "        elif j == 'translator_administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],12))\n",
    "        elif j == 'wikidata_staff':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],13))\n",
    "\n",
    "df_rights=pd.DataFrame(data_lst, columns =['name','username_type','acount_age', 'number_of_edits', 'total_posts','talking_type','status','rights'])\n",
    "df_rights.to_csv('registered_editors_coded_access_levels.csv',index=None, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import csv\n",
    "import json\n",
    "\n",
    "files_path='discussions/'# the directory where we store thw csv with all discussions separated into posts\n",
    "save_path='asking_responding_channels/'# JSON files with all editors. Each has the number of times their post was first in a thread, and their posts was not first in the thread\n",
    "\n",
    "data={} # to store the edges as tuples\n",
    "#read files for the different discussion channels\n",
    "for file in os.listdir(str(files_path)):\n",
    "\n",
    "    df=pd.read_csv(str(files_path)+str(file), encoding='utf-8')# load data for the file\n",
    "    subjects=df['thread_title'].unique() # keep using thread titles\n",
    "\n",
    "    #create edges based on the thread title\n",
    "    for subject in subjects:\n",
    "        print(file, subject)\n",
    "        if subject!='No subject': # if the thread has no title (i.e., 'No title') process based on the page title\n",
    "            thread=df.loc[df['thread_title']==subject]# store the rows of posts from the same thread\n",
    "            # for every name create a tuple with the other names in the thread\n",
    "            thread.reset_index(drop=True, inplace=True)\n",
    "            for row in range(len(thread)):\n",
    "                if row==0:\n",
    "                    if thread.loc[row,'urername'] in data.keys():\n",
    "                        data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0]+1,data[thread.loc[row,'urername']][1]]\n",
    "                    else:\n",
    "                        data[thread.loc[row,'urername']]=[1,0]\n",
    "                else:\n",
    "                    if thread.loc[row,'urername'] in data.keys():\n",
    "                        data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0],data[thread.loc[row,'urername']][1]+1]\n",
    "                    else:\n",
    "                        data[thread.loc[row,'urername']]=[0,1]\n",
    "        else:\n",
    "            no_title_rows=df.loc[df['thread_title']==subject]# store the posts with 'No title' as thread title\n",
    "            pages=no_title_rows['page_title'].unique() # keep unique pages names\n",
    "            for page in pages:\n",
    "                print(file, subject, page)\n",
    "                thread=df.loc[(df['page_title']==page) & (df['thread_title']==subject)]\n",
    "                thread.reset_index(drop=True, inplace=True)\n",
    "                for row in range(len(thread)):\n",
    "                    if row==0:\n",
    "                        if thread.loc[row,'urername'] in data.keys():\n",
    "                            data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0]+1,data[thread.loc[row,'urername']][1]]\n",
    "                        else:\n",
    "                            data[thread.loc[row,'urername']]=[1,0]\n",
    "                    else:\n",
    "                        if thread.loc[row,'urername'] in data.keys():\n",
    "                            data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0],data[thread.loc[row,'urername']][1]+1]\n",
    "                        else:\n",
    "                            data[thread.loc[row,'urername']]=[0,1]\n",
    "\n",
    "    \n",
    "\n",
    "    with open(str(save_path)+str(file[:-4])+'.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp,ensure_ascii=False)\n",
    "\n",
    "    data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def mergeDictionary(dict_1, dict_2):\n",
    "   dict_3 = {**dict_1, **dict_2}\n",
    "   for key, value in dict_3.items():\n",
    "       if key in dict_1 and key in dict_2:\n",
    "               dict_3[key] = [sum(x) for x in zip(value, dict_1[key])]\n",
    "   return dict_3\n",
    "\n",
    "files_path='asking_responding_channels/'\n",
    "\n",
    "dict_3={} # to store the edges as tuples\n",
    "#read files for the different discussion channels\n",
    "\n",
    "for file in os.listdir(str(files_path)):\n",
    "   # Opening JSON file\n",
    "   f = open(str(files_path)+str(file), encoding='utf-8')\n",
    "   data1 = json.load(f)\n",
    "   dict_3 = mergeDictionary(dict_3, data1)\n",
    "\n",
    "   # print('1: ',[d[0] for d in dict_3.values()])\n",
    "   # print('2: ', [d[1] for d in dict_3.values()])\n",
    "   # print([sum(x) for x in zip([d[0] for d in dict_3.values()], [d[1] for d in dict_3.values()] )])\n",
    "df=pd.DataFrame({'name':dict_3.keys(),'asking':[d[0] for d in dict_3.values()], 'responding':[d[1] for d in dict_3.values()], 'total_posts':[sum(x) for x in zip([d[0] for d in dict_3.values()], [d[1] for d in dict_3.values()] )]})\n",
    "df.to_csv('asking_responding.csv',encoding='utf-8', index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('asking_responding.csv')\n",
    "\n",
    "status_threashold=19\n",
    "\n",
    "status=[]\n",
    "for a, r in zip(df.asking, df.responding):\n",
    "    if a>r:\n",
    "        status.append(1)# this is asking \n",
    "    elif a<r:\n",
    "        status.append(0)# this is for responding\n",
    "    else: status.append(2)# this is for equal\n",
    "\n",
    "save_df=pd.concat([df,pd.DataFrame({'talking_type':status})],axis=1)\n",
    "save_df.to_csv('username_talking_type.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
