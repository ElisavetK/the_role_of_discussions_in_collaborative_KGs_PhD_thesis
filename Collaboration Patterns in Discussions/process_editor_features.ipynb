{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asign IDs to usernames, posts, threads, and pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usernames\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('path_to_file_with_username_inflo', encoding='utf-8')\n",
    "df_id=pd.DataFrame(columns=['UID','username','user_type','registration','rights','asking','responding','num_posts', 'talking_type'])\n",
    "for row in range(len(df)):\n",
    "    df_id.loc[row,'UID']='U'+str(row)\n",
    "    df_id.iloc[row,1:2]=df.iloc[row,2:3]\n",
    "    df_id.iloc[row,3:]=df.iloc[row,5:]\n",
    "df_id.to_csv(\"path_to_save\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post, thread, and page ids\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "UID_df=pd.read_csv(\"username_ids.csv\",encoding='utf-8')\n",
    "\n",
    "path='path_to_directory_with_post_info'\n",
    "\n",
    "p_id=0\n",
    "pt_id=0\n",
    "t_id=0\n",
    "for file in os.listdir(str(path)):\n",
    "\n",
    "    # file='items.csv'\n",
    "    df=pd.read_csv(str(path)+str(file),encoding='utf-8')\n",
    "\n",
    "    # dropping the rows having NaN values\n",
    "    df = df.dropna()\n",
    "    # To reset the indices\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df_id=pd.DataFrame(columns=['PID','post','PTID','page_title','TID','thread_title','UID','username','timestamp','answer2post'])\n",
    "\n",
    "    # set page_title ids\n",
    "    pt_uniqies=df['page_title'].unique()\n",
    "    pt_ids_df=None\n",
    "    pt_ids_df=pd.DataFrame({'title':pt_uniqies,'id':['PT'+str(x) for x in range(pt_id,pt_id+len(pt_uniqies))]})\n",
    "    pt_id=pt_id+len(pt_uniqies)\n",
    "\n",
    "    #set thread_title ids\n",
    "    t_ids_df=pd.DataFrame()\n",
    "    for pt in pt_uniqies:\n",
    "        df_pt=df.loc[df['page_title']==pt]\n",
    "        t_uniqies=df_pt['thread_title'].unique()\n",
    "        t_ids_df1=None\n",
    "        t_ids_df1=pd.DataFrame({'title':t_uniqies,'id':['T'+str(x) for x in range(t_id,t_id+len(t_uniqies))], 'page':[pt]*len(t_uniqies)})\n",
    "        t_id=t_id+len(t_uniqies)\n",
    "        t_ids_df=pd.concat([t_ids_df,t_ids_df1])\n",
    "\n",
    "    for row in range(len(df)):\n",
    "        print(file, row)\n",
    "        df_id.loc[row,'PID']='P'+str(p_id+row)\n",
    "        df_id.loc[row,'post']=df.loc[row,'post']\n",
    "        df_id.loc[row,'PTID']=pt_ids_df.loc[pt_ids_df['title']==df.loc[row,'page_title'],'id'].values[0]\n",
    "        df_id.loc[row,'page_title']=df.loc[row,'page_title']\n",
    "        df_id.loc[row,'TID']=t_ids_df.loc[((t_ids_df['title']==df.loc[row,'thread_title'])&(t_ids_df['page']==df.loc[row,'page_title'])),'id'].values[0]\n",
    "        df_id.loc[row,'thread_title']=df.loc[row,'thread_title']\n",
    "        df_id.loc[row,'UID']=UID_df.loc[UID_df['username']==df.loc[row,'urername'],'UID'].values[0]\n",
    "        df_id.loc[row,'username']=df.loc[row,'urername']\n",
    "        df_id.loc[row,'timestamp']=df.loc[row,'timestamp']\n",
    "        df_id.loc[row,'answer2post']=df.loc[row,'answer2post']\n",
    "        \n",
    "\n",
    "\n",
    "        # df_id.iloc[row,1:2]=df.iloc[row,2:3]\n",
    "        # df_id.iloc[row,3:]=df.iloc[row,5:]\n",
    "\n",
    "    p_id=p_id+row+1\n",
    "    df_id.to_csv('path_to_save_directory'+str(file), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editors' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration date\n",
    "\n",
    "We used a Wikidata API https://www.wikidata.org/w/api.php?action=help&modules=query%2Busers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the info\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://www.wikidata.org/w/api.php\"#info about the features you can extract https://www.wikidata.org/w/api.php?action=help&modules=query%2Busers\n",
    "\n",
    "\n",
    "editors_count=0\n",
    "line_count=0\n",
    "\n",
    "\n",
    "users = pd.DataFrame(columns=['userid','name','editcount', 'registration','rights','name_post'])#create a data frame with the features you choose\n",
    "\n",
    "editors_count=0\n",
    "#path to editors names\n",
    "with open('known_names_APIinput.txt', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        \n",
    "        #editors_in_each_line_count=0\n",
    "        for part in l.split(\"|\"):\n",
    "            editors_count += 1\n",
    "            print('count: ',editors_count, \" editor: \", part)\n",
    "            # line_count+=1\n",
    "            #editors_in_each_line_count+=1\n",
    "            \n",
    "            PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"users\",\n",
    "            \"ususers\": part,\n",
    "            # \"usprop\": \"blockinfo|editcount|registration|rights\"} #you choose features to extarct\n",
    "            \"usprop\": \"editcount|registration|rights\"} #you choose features to extarct\n",
    "            \n",
    "\n",
    "            \n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "            DATA = R.json()\n",
    "            \n",
    "            USERS = DATA[\"query\"][\"users\"]\n",
    "            # print(USERS[0])\n",
    "            row=USERS[0]\n",
    "            row['name_post']=part  \n",
    "            users = users.append(row, ignore_index=True)    \n",
    "            #unknown_users=pd.DataFrame(columns=['names'])  \n",
    "            # for u in USERS:\n",
    "            #     editors_count+=1\n",
    "            #     #if ('invalid' not in u.keys()) and ('missing' not in u.keys()):\n",
    "            #         #print(str(u[\"name\"]) + \" has \" + str(u[\"editcount\"]) + \" edits.\")\n",
    "            #         #print(str(u[\"name\"]) + ' with id ' + str(u['userid']))\n",
    "            #         #print(u['groupmemberships'])\n",
    "            #     users = users.append(u, ignore_index=True)\n",
    "            #     #else:unknown_users=unknown_users.append(u, ignore_index=True)\n",
    "                \n",
    "            \n",
    "            #path to save       \n",
    "            users.to_csv('known_names_info_output_v2.csv', encoding='utf-8', index=False)\n",
    "            #unknown_users.to_csv('/mnt/data/elisavetk/Theme_2/Post_Graph/unknown_editor_info.csv', encoding='utf-8', index=False,  mode='a', header=False)\n",
    "\n",
    "#print(editors_count)\n",
    "\n",
    "'''if editors_in_each_line_count == 49:\n",
    "    break'''\n",
    "#break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors' type \n",
    "\n",
    "registered, unregistered, unknown names, bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#load usernames found in discussions\n",
    "df=pd.read_csv('unique_usernames_found_in_discussions.csv', encoding='utf-8')\n",
    "u_df_lst=df['username'].tolist()\n",
    "\n",
    "# store known usernames. it is the merge of the above two\n",
    "u_common=df\n",
    "\n",
    "# store anonymou_usernames\n",
    "u_anonymous=[name for name in u_df_lst if 'Anonymous_username_'in str(name)]\n",
    "\n",
    "# store Unregistered users\n",
    "pattern1=r\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "pattern2=r\"(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "u_IP4=[name for name in u_df_lst if bool(re.match(pattern1,str(name)))]\n",
    "u_IP6=[name for name in u_df_lst if bool(re.match(pattern2,str(name)))]\n",
    "\n",
    "\n",
    "# len(u_common)+len(u_anonymous)+len(u_IP4)+len(u_IP6)\n",
    "known_names=u_common['username'].to_list()+u_anonymous+u_IP4+u_IP6 # registered and unregistered editors\n",
    "unknown_names=[name for name in u_df_lst if str(name) not in known_names] # unknwon names\n",
    "\n",
    "\n",
    "\n",
    "#check how many names are unknown names\n",
    "len(unknown_names)*100/len(u_df_lst)\n",
    "# the uknown names are 5% of the total names I found in discussions\n",
    "\n",
    "# create df with the username info for the three files\n",
    "u_data1_df=data(known_names)\n",
    "# u_data2_df=data(u_data2)\n",
    "u_data3_df=data(unknown_names)\n",
    "\n",
    "# store data anonymou_usernames\n",
    "u_anonymous=[name for name in df['username'] if 'Anonymous_username_'in str(name)]\n",
    "u_anonymous_data={name:[None,name,'anonymous_username',None, None, None] for name in u_anonymous}\n",
    "u_anonymous_df = pd.DataFrame([v for k, v in u_anonymous_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "\n",
    "# store data Unregistered users\n",
    "pattern1=r\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "pattern2=r\"(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "u_IP4=[name for name in df['username'] if bool(re.match(pattern1,str(name)))]\n",
    "u_IP4_data={name:[None,name,'unregistered',None, None, None] for name in u_IP4}\n",
    "u_IP4_df = pd.DataFrame([v for k, v in u_IP4_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "u_IP6=[name for name in df['username'] if bool(re.match(pattern2,str(name)))]\n",
    "u_IP6_data={name:[None,name,'unregistered',None, None, None] for name in u_IP6}\n",
    "u_IP6_df = pd.DataFrame([v for k, v in u_IP6_data.items()], columns=['userid', 'name', 'username_type', 'editcount_API','registration','rights'])\n",
    "\n",
    "\n",
    "# df_full=pd.concat([u_data1_df,u_data2_df,u_data3_df,u_anonymous_df,u_IP4_df,u_IP6_df])\n",
    "df_full=pd.concat([u_data1_df,u_data3_df,u_anonymous_df,u_IP4_df,u_IP6_df])\n",
    "df_full.to_csv('username_type.csv',encoding='utf-8',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"username_type.csv\", encoding='utf-8')\n",
    "df_bots=pd.read_csv('bot_names.csv', encoding='utf-8')\n",
    "bot_name=df_bots['Name'].to_list()\n",
    "\n",
    "for i in range(len(df['username_type'])):\n",
    "     if df.loc[i,'name'] in bot_name:\n",
    "          df.loc[i,'username_type']='bot'\n",
    "\n",
    "df.to_csv('username_type.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors' edit history\n",
    "\n",
    "Process edit history to measure account age (the difference between registare date and last edit), max gaps of inacitvity (last edit and today), and number of edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "directory_path='username_edits/'# these are files with (username, timestamp) for editors' edits\n",
    "\n",
    "# last timestamp for the archived data\n",
    "today=pd.to_datetime('2023-03-02T00:00:00Z', format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "\n",
    "#load username information\n",
    "all_users=pd.read_csv('username_info_v2.csv',encoding='utf-8')\n",
    "#keep registered users\n",
    "#create a lsit with registered editors\n",
    "registered=all_users.loc[all_users['username_type'] == 'registered']\n",
    "# registered=registered1.head(1000)\n",
    "\n",
    "#lists to store data\n",
    "last_edit=[]\n",
    "acount_age=[]\n",
    "max_gap=[]\n",
    "last_today=[]\n",
    "number_of_edits=[]\n",
    "false_edits_names=[]\n",
    "mean_gap=[]\n",
    "median_gap=[]\n",
    "\n",
    "#for every registered user find last edit timestamp, acount age, max inactive gap, number of edits timestamps\n",
    "num=0\n",
    "for name in registered['name']:\n",
    "    num +=1\n",
    "    print(num, name)\n",
    "    # name='Ilmari Karonen'\n",
    "    # convert registration date to datetime type\n",
    "    registrasion_date=pd.to_datetime(registered.loc[registered['name']==name,'registration'], format='%Y-%m-%dT%H:%M:%SZ').to_frame()\n",
    "    # print(registrasion_date.iloc[0,0])\n",
    "\n",
    "    #load file with edit timestamps\n",
    "    #create a list with the files of users in the username_edits directory\n",
    "    onlyfiles = [f[:-4] for f in listdir('username_edits/') if isfile(join('/mnt/data/elisavetk/Wikidata_create_dataset/Create_csv_data_files/username_edits/', f))]\n",
    "\n",
    "    #check if the registered user has edits\n",
    "    if name in onlyfiles:\n",
    "        f=open(str(directory_path)+str(name)+'.txt','r')\n",
    "        data=f.readlines()\n",
    "\n",
    "        data_df=pd.DataFrame({'timestamp':data})#convert the list of timestamps to dataframe\n",
    "        dates=pd.to_datetime(data_df['timestamp'], format='%Y-%m-%dT%H:%M:%SZ\\n').to_frame()#convert the strings to date time\n",
    "\n",
    "        # check if there are edits with timestamp before 10/2012 and if yes, remove the timestamps and store the editors' names\n",
    "        false_edits=dates[dates['timestamp']<'2012-09-30T00:00:00']\n",
    "        if len(false_edits)>0:\n",
    "            false_edits_names.append(name)\n",
    "            # #find which rows have true, meaning which rows we want to extract\n",
    "            # num_rows_lst=dates.index[dates['timestamp']<'2012-09-30T00:00:00'].tolist()\n",
    "            #extract the rows from the edit hisotry data\n",
    "            dates=dates.loc[~dates.index.isin(false_edits.index)]\n",
    "\n",
    "\n",
    "        dates.sort_values(by='timestamp', ascending = True, inplace = True)# ascending order to date time\n",
    "            \n",
    "        #check if the editor has one or more edits\n",
    "        if len(dates)>1: \n",
    "            date_diff=dates['timestamp'].diff()/ np.timedelta64(1, 'M')# find the time intervals in months between edits\n",
    "            \n",
    "            # store the data\n",
    "            last_edit.append(dates.iloc[-1].values[0])\n",
    "            age=(dates.iloc[-1]-registrasion_date.iloc[0,0])/ np.timedelta64(1, 'M')\n",
    "            acount_age.append(age.values[0])\n",
    "            max_gap.append(np.nanmax(date_diff.to_frame()))\n",
    "            mean_gap.append(date_diff.to_frame().mean().values[0])\n",
    "            median_gap.append(date_diff.to_frame().median().values[0])\n",
    "            today_diff=(today-dates.iloc[-1])/ np.timedelta64(1, 'M')\n",
    "            last_today.append(today_diff.values[0])\n",
    "            number_of_edits.append(len(dates))\n",
    "        else:\n",
    "            #store the data\n",
    "            last_edit.append(dates.iloc[-1].values[0])\n",
    "            age=(dates.iloc[-1]-registrasion_date.iloc[0,0])/ np.timedelta64(1, 'M')\n",
    "            acount_age.append(age.values[0])\n",
    "            max_gap.append(0)\n",
    "            mean_gap.append(0)\n",
    "            median_gap.append(0)\n",
    "            today_diff=(today-dates.iloc[-1])/ np.timedelta64(1, 'M')\n",
    "            last_today.append(today_diff.values[0])\n",
    "            number_of_edits.append(len(dates))\n",
    "    else:\n",
    "        last_edit.append(0)\n",
    "        acount_age.append(0)\n",
    "        max_gap.append(0)\n",
    "        mean_gap.append(0)\n",
    "        median_gap.append(0)\n",
    "        today_diff=0\n",
    "        last_today.append(0)\n",
    "        number_of_edits.append(0)\n",
    "\n",
    "\n",
    "#save the full data\n",
    "my_data=pd.DataFrame({'last_edit':last_edit,'acount_age':acount_age,'max_gap':max_gap, 'mean_gap':mean_gap,'median_gap':median_gap,'last_today':last_today,'number_of_edits':number_of_edits})\n",
    "save_data=pd.concat([registered,my_data.set_index(registered.index)], axis=1)\n",
    "save_data.to_csv('username_info.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "#save the false edits names\n",
    "my_data_names=pd.DataFrame({'names':false_edits_names})\n",
    "my_data_names.to_csv('false_edits.csv', encoding='utf-8', index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status\n",
    "\n",
    "We measured above the difference between the last edit and today for each editor. We set a thresfold at 23 months for active and inactive editors. This threshold was measured using the percentile of max gaps of inactivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure percentile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('username_stats.csv') # this includes the max_gap\n",
    "\n",
    "status_threashold=np.percentile(df['max_gap'], 85)\n",
    "status_threashold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('username_stats.csv')\n",
    "\n",
    "status_threashold=23\n",
    "\n",
    "status=[]\n",
    "for last, num in zip(df.last_today, df.number_of_edits):\n",
    "    if last<status_threashold and num!=0:\n",
    "        status.append(1)# this is for active editors\n",
    "    else:status.append(0)# this is for inactive editors\n",
    "\n",
    "save_df=pd.concat([df,pd.DataFrame({'status':status})],axis=1)\n",
    "save_df.to_csv('username_status.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Level\n",
    "\n",
    "We extracted and saved the usernames of the different access levels from the Wikidata interface https://www.wikidata.org/wiki/Special:ListUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='files_access_level_usernames/'\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file[0]!='.':\n",
    "        print(file)\n",
    "        df_file=pd.read_csv(str(path)+str(file), encoding='utf-8')\n",
    "        names_list=df_file['clean_name'].to_list()\n",
    "        for name in df['name']:\n",
    "            if name in names_list:\n",
    "                label=df.loc[df['name']==name,'rights_label'].to_list()\n",
    "                if 'editor' in label: label.remove('editor')\n",
    "                label.append(file[:-4])\n",
    "                # label=str(label)+';'+str(file[:-4])\n",
    "                df.loc[df['name']==name,'rights_label']=';'.join(label)\n",
    "\n",
    "df.to_csv('registered_editors_access_levels.csv', index=None,encoding='utf-8')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1=editor\n",
    "14=acount creator\n",
    "15=bot \n",
    "4=buraucrats\n",
    "5=checkusers\n",
    "6=confirmed users\n",
    "7=flooders\n",
    "8= interface administartors\n",
    "9= IP exeptions\n",
    "10=rollblockers\n",
    "11=suppresors\n",
    "12=translator administrators\n",
    "13= wikidata staff\n",
    "2= property creator\n",
    "3= administrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('registered_editors_access_levels.csv', encoding='utf-8')\n",
    "\n",
    "data_lst=[]\n",
    "for i in range(len(df)):\n",
    "    labels=df.loc[i,'rights_label']\n",
    "    all_labels=labels.split(';')\n",
    "    for j in all_labels:\n",
    "        if j == 'editor':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],1))\n",
    "        elif j == 'acount_creator':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],14))\n",
    "        elif j == 'administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],3))\n",
    "        elif j == 'bots':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],15))\n",
    "        elif j == 'buraucrats':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],4))\n",
    "        elif j == 'checkusers':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],5))\n",
    "        elif j == 'confirmed_users':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],6))\n",
    "        elif j == 'flooders':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],7))\n",
    "        elif j == 'interface_administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],8))\n",
    "        elif j == 'IP_exeptions':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],9))\n",
    "        elif j == 'property_creator':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],2))\n",
    "        elif j == 'rollblockers':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],10))\n",
    "        elif j == 'suppresors':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],11))\n",
    "        elif j == 'translator_administrators':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],12))\n",
    "        elif j == 'wikidata_staff':\n",
    "            data_lst.append((df.loc[i,'name'],df.loc[i,'username_type'],df.loc[i,'acount_age'],df.loc[i,'number_of_edits'],df.loc[i,'total_posts'],df.loc[i,'talking_type'],df.loc[i,'status'],13))\n",
    "\n",
    "df_rights=pd.DataFrame(data_lst, columns =['name','username_type','acount_age', 'number_of_edits', 'total_posts','talking_type','status','rights'])\n",
    "df_rights.to_csv('registered_editors_coded_access_levels.csv',index=None, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import csv\n",
    "import json\n",
    "\n",
    "files_path='discussions/'# the directory where we store thw csv with all discussions separated into posts\n",
    "save_path='asking_responding_channels/'# JSON files with all editors. Each has the number of times their post was first in a thread, and their posts was not first in the thread\n",
    "\n",
    "data={} # to store the edges as tuples\n",
    "#read files for the different discussion channels\n",
    "for file in os.listdir(str(files_path)):\n",
    "\n",
    "    df=pd.read_csv(str(files_path)+str(file), encoding='utf-8')# load data for the file\n",
    "    subjects=df['thread_title'].unique() # keep using thread titles\n",
    "\n",
    "    #create edges based on the thread title\n",
    "    for subject in subjects:\n",
    "        print(file, subject)\n",
    "        if subject!='No subject': # if the thread has no title (i.e., 'No title') process based on the page title\n",
    "            thread=df.loc[df['thread_title']==subject]# store the rows of posts from the same thread\n",
    "            # for every name create a tuple with the other names in the thread\n",
    "            thread.reset_index(drop=True, inplace=True)\n",
    "            for row in range(len(thread)):\n",
    "                if row==0:\n",
    "                    if thread.loc[row,'urername'] in data.keys():\n",
    "                        data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0]+1,data[thread.loc[row,'urername']][1]]\n",
    "                    else:\n",
    "                        data[thread.loc[row,'urername']]=[1,0]\n",
    "                else:\n",
    "                    if thread.loc[row,'urername'] in data.keys():\n",
    "                        data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0],data[thread.loc[row,'urername']][1]+1]\n",
    "                    else:\n",
    "                        data[thread.loc[row,'urername']]=[0,1]\n",
    "        else:\n",
    "            no_title_rows=df.loc[df['thread_title']==subject]# store the posts with 'No title' as thread title\n",
    "            pages=no_title_rows['page_title'].unique() # keep unique pages names\n",
    "            for page in pages:\n",
    "                print(file, subject, page)\n",
    "                thread=df.loc[(df['page_title']==page) & (df['thread_title']==subject)]\n",
    "                thread.reset_index(drop=True, inplace=True)\n",
    "                for row in range(len(thread)):\n",
    "                    if row==0:\n",
    "                        if thread.loc[row,'urername'] in data.keys():\n",
    "                            data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0]+1,data[thread.loc[row,'urername']][1]]\n",
    "                        else:\n",
    "                            data[thread.loc[row,'urername']]=[1,0]\n",
    "                    else:\n",
    "                        if thread.loc[row,'urername'] in data.keys():\n",
    "                            data[thread.loc[row,'urername']]=[data[thread.loc[row,'urername']][0],data[thread.loc[row,'urername']][1]+1]\n",
    "                        else:\n",
    "                            data[thread.loc[row,'urername']]=[0,1]\n",
    "\n",
    "    \n",
    "\n",
    "    with open(str(save_path)+str(file[:-4])+'.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp,ensure_ascii=False)\n",
    "\n",
    "    data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def mergeDictionary(dict_1, dict_2):\n",
    "   dict_3 = {**dict_1, **dict_2}\n",
    "   for key, value in dict_3.items():\n",
    "       if key in dict_1 and key in dict_2:\n",
    "               dict_3[key] = [sum(x) for x in zip(value, dict_1[key])]\n",
    "   return dict_3\n",
    "\n",
    "files_path='asking_responding_channels/'\n",
    "\n",
    "dict_3={} # to store the edges as tuples\n",
    "#read files for the different discussion channels\n",
    "\n",
    "for file in os.listdir(str(files_path)):\n",
    "   # Opening JSON file\n",
    "   f = open(str(files_path)+str(file), encoding='utf-8')\n",
    "   data1 = json.load(f)\n",
    "   dict_3 = mergeDictionary(dict_3, data1)\n",
    "\n",
    "   # print('1: ',[d[0] for d in dict_3.values()])\n",
    "   # print('2: ', [d[1] for d in dict_3.values()])\n",
    "   # print([sum(x) for x in zip([d[0] for d in dict_3.values()], [d[1] for d in dict_3.values()] )])\n",
    "df=pd.DataFrame({'name':dict_3.keys(),'asking':[d[0] for d in dict_3.values()], 'responding':[d[1] for d in dict_3.values()], 'total_posts':[sum(x) for x in zip([d[0] for d in dict_3.values()], [d[1] for d in dict_3.values()] )]})\n",
    "df.to_csv('asking_responding.csv',encoding='utf-8', index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('asking_responding.csv')\n",
    "\n",
    "status_threashold=19\n",
    "\n",
    "status=[]\n",
    "for a, r in zip(df.asking, df.responding):\n",
    "    if a>r:\n",
    "        status.append(1)# this is asking \n",
    "    elif a<r:\n",
    "        status.append(0)# this is for responding\n",
    "    else: status.append(2)# this is for equal\n",
    "\n",
    "save_df=pd.concat([df,pd.DataFrame({'talking_type':status})],axis=1)\n",
    "save_df.to_csv('username_talking_type.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
