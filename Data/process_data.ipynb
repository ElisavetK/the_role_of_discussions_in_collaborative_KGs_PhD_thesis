{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate threads and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import json\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "#this is to extent the size of the reading csv cell\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "file_path='path_to_raw+data'\n",
    "save_path_trash='path_to_save_data'\n",
    "\n",
    "\n",
    "def UTC_separation(thread):\n",
    "    posts_lst=[]\n",
    "    posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',thread)# split the posts based on the timestamp\n",
    "    for i in range(0,len(posts)-1, 2):\n",
    "        posts_lst.append(posts[i]+posts[i+1])# connects post with timestamp because the splot separates them\n",
    "    \n",
    "    return posts_lst\n",
    "\n",
    "#to separate discussions with no title\n",
    "def thread_without_title(temp, page_title, save_path_data):\n",
    "    if ((temp[0:2]=='{{') and (('documentation' in temp) or ('Documentation' in temp))) or ((temp[0:2]=='{{') and (('Interwiki conflict' in temp) or ('interwiki conflict' in temp))) or ('#REDIRECT' in temp): # checks if the text includes only documentation, not discussions\n",
    "         trash=pd.DataFrame({'page_title':[page_title], 'text':temp})\n",
    "         trash.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))\n",
    "    else:\n",
    "        thread_title='No subject'\n",
    "        posts_lst = UTC_separation(temp)\n",
    "        #store posts with the corresponding sibject\n",
    "        df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "        #save to csv \n",
    "        df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "\n",
    "\n",
    "def thread_with_title(titles, temp, page_title, save_path_data):\n",
    "    for j in range(len(temp)):\n",
    "        if (temp[j] in titles):\n",
    "            if (j+1)==(len(temp)):\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = []\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "            else:\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = UTC_separation(temp[j+1])\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "        \n",
    "    \n",
    "\n",
    "#function to create a file with the separated posts  \n",
    "def separate_discussions(text, page_title, save_path_data):\n",
    "\n",
    "    titles = re.findall('==(.*)==', text)# find the titles in the discussion  \n",
    "    titles=[s.replace('=', '') for s in titles]# removes unecessery = in the title\n",
    "    \n",
    "    \n",
    "    #titles=list(set(titles)) \n",
    "    temp= text.split('==') # split the text based on == , includided in the title\n",
    "    temp = [x for x in temp if x != ''] # stores the splited parts\n",
    "    temp=[s.strip('=') for s in temp]# removes unecessery = in the title\n",
    "\n",
    "    # ---Step 1: there is no title in the page\n",
    "    if titles==[]: # check if there is title in the begining of the page\n",
    "        thread_without_title(temp[0], page_title, save_path_data) \n",
    "    else:\n",
    "        if temp[0] not in titles:\n",
    "            thread_without_title(temp[0], page_title, save_path_data)# ---Step 2: the first thread do not have title \n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ignores the first if does not have a title and process the rest threads\n",
    "        else:\n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ---Step 3: a;; threads have titles \n",
    "\n",
    "\n",
    "\n",
    "file_lst=['items.csv','properties.csv']\n",
    "    \n",
    "\n",
    "for file in file_lst:\n",
    "    save_path_data='path'+str(file) # path to save posts and threads\n",
    "    data=pd.read_csv(str(file_path)+str(file), encoding='utf-8')# read file\n",
    "    for row in range(len(data)):# read row by row (i.e., page by page)\n",
    "        print(row)\n",
    "        # print(data.loc[row,'page_title'])\n",
    "        # print(data.loc[row,'text'])\n",
    "        page_title=data.loc[row,'page_title']\n",
    "        if isinstance(data.loc[row,'text'],str):# check of the row is nan. If it is nan the type is float not str\n",
    "            separate_discussions(data.loc[row,'text'], page_title, save_path_data)\n",
    "        else:\n",
    "            empty=pd.DataFrame({'page_title':[page_title], 'text':'empty'})\n",
    "            empty.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract username and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# find usernmae based on [[User talk:]] pattern\n",
    "def find_username_User_talk(text):\n",
    "    pattern=re.compile(r\"(\\[)?(User:|User talk:|user:|user talk:|User Talk:|User_talk:|User_Talk:|user_talk:|Utente:|Usuário:|Utilisateur:|Utilizator:|Usuari Discussió:|Usuario discusión:|Usuario:|Usuario Discusi\\u00f3n:|kullanıcı:|Kullan\\u0131c\\u0131:|:USER TALK:|:USER:|U:|u:|Benutzer:|Benutzer Diskussion:|Special:Contributions/|welcominguser=|Discussão:|Dyskusja_Wikipedysty:|사용자토론:)(.*?)(\\||/|\\])\") # the regular expression I need to extract \n",
    "    usernames=pattern.finditer(text)\n",
    "    #search for the username\n",
    "    names=[]        \n",
    "    for match in usernames:\n",
    "        #print(match.group(2))\n",
    "        names.append([match.group(3),match.end()])\n",
    "        \n",
    "    if len(names)>0:\n",
    "        max_loc=max([sublist[1] for sublist in names])\n",
    "        name=[sublist[0] for sublist in names if sublist[1]==max_loc]\n",
    "    else: name=[]\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#funstion to extract username from the post\n",
    "def extract_info(raw_text, anon):\n",
    "    timestamp='No date'\n",
    "    username= 'Unsigned post'\n",
    "    text1=raw_text.replace('‎','')\n",
    "    t_split=text1.split('(UTC)') # this is mainly for property proposal, because after (UTC) it includes remaining punctuation\n",
    "    t_connect1=[str(part)+'(UTC)' for part in t_split[ :-1]] # remove the final part with punctuaton\n",
    "    t_connect='(UTC)'.join(t_connect1) # connect again the text\n",
    "    \n",
    "    if t_connect.endswith('(UTC)'):# if UTC in not in the text, the post does not have a signature or timestmap so the usernmae will be assigned as unsigned and the timestamp with no date\n",
    "        peaces=re.split(\"(\\d+:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\", t_connect)\n",
    "        if len(peaces)>1:\n",
    "            timestamp=peaces[1]\n",
    "            u_user_talk=find_username_User_talk(peaces[0])# step 1--- detect the pattern [[user talk:]] to find the username\n",
    "            \n",
    "            if len(u_user_talk)<1:# if step 1 do not work\n",
    "                u_from_list= [name for name in username_lst if name in peaces[0]]# step 2 ----- check the usernmae list\n",
    "                \n",
    "                for name in u_from_list:\n",
    "                    text=peaces[0].replace('(talk) ','')\n",
    "                    u_peaces=re.split(name,text)\n",
    "                    if len(u_peaces[-1])<10:\n",
    "                        print(name)\n",
    "                        return timestamp, name, anon\n",
    "                if len(u_from_list)<1 or username== 'Unsigned post':# if step 1 and 2 do not work the username will be assigned as anonymous\n",
    "                    anon += 1\n",
    "                    username= 'Anonymous_username_'+str(anon)\n",
    "                    print(username)\n",
    "                    return timestamp, username, anon\n",
    "                    \n",
    "            else:\n",
    "                print(u_user_talk)\n",
    "                return timestamp, u_user_talk[0], anon\n",
    "        else:\n",
    "            print(username)\n",
    "            return timestamp, username, anon    \n",
    "    else:\n",
    "        print(username)\n",
    "        return timestamp, username, anon\n",
    "\n",
    "    \n",
    "\n",
    " #---------RUN-----------\n",
    "\n",
    "\n",
    "file_path='path_to_directory__files_for_process'\n",
    "save_path='path_to_directory_to_save_files'\n",
    "\n",
    "# load the list with usernames we have gathered during the dump process \n",
    "# read usernames \n",
    "my_file = open(\"path_to_username_list\", \"r\", encoding='utf-8')\n",
    "data = my_file.read()\n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "username_lst = data.split(\"\\n\")\n",
    "my_file.close()\n",
    "# #separate usernmaes based on length because there are usernmaes with one character and confuse the correct detection\n",
    "# usernames_short=[n for n in username_lst if len(n)<=2]\n",
    "# usernames_long=[n for n in username_lst if len(n)>3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "anon=0 # this is to give names in case we can not recognise a username\n",
    "for dump in os.listdir(str(file_path)):\n",
    "    # dump='property_proposal_all.csv'\n",
    "    df=pd.read_csv(str(file_path)+str(dump), encoding='utf-8')\n",
    "    for row in range(len(df)):\n",
    "        print(dump, row)\n",
    "        # test=\"=== {{TranslateThis | anchor = en| en = Kinsky–Halm Catalogue <!-- |xx = property names in some other languages -->}} ==={{Property proposal|status\t\t\t= 7161|description\t\t= {{TranslateThis | en = Number in the Kinsky–Halm Catalogue of the works of Ludwig van Beethoven, which do not have opus numbers, or are fragmentary.<!-- | xx = descriptions in other languages --> }}|subject item           = {{Q|616043}}|infobox parameter\t= [[en:catalogue:infobox musical composition]]<!-- Wikipedia infobox parameters, if any; ex: \"population\" in [[:en:template:infobox settlement]] -->|datatype\t\t= external-id<!-- put datatype here (one of: item, external-id, string, media, coordinates, monolingual text, multilingual text, time, URL, number, geo-shape, tabular, lexeme, form, sense, mathematical expression, or a possibly new one) -->|domain\t\t\t= <!-- entity type (item, property, lexeme, form, sense) and types of items that may bear this property; give QIDs if possible -->|allowed values\t\t= \\d+<!-- type of linked items (Q template or text), string pattern (regex if possible), list or range of allowed values, etc. -->|allowed units          = <!-- units that are allowed for values of this property -->|source\t\t\t= https://en.wikipedia.org/wiki/WoO <!-- external reference URL, Wikipedia list article, etc. --><!-- you should provide 3 examples at least-->|example 1\t\t= {{Q|8076022}} → 123 <!-- {{Q|1}} → value -->|example 2\t\t= {{Q|166550}} → 4|example 3\t\t= {{Q|166173}} → 6|example 4\t\t= <!--  if convenient, include more examples by following the same pattern: |example ...\t\t=  {{Q|1}} → value-->|planned use            = Add to Authority control<!-- what use do you plan to make of the property within the next month -->|number of ids          = <!-- for identifiers, the number of ids available from the source -->|expected completeness  = Q21873974 <!-- for identifiers, the coverage we can hope for in Wikidata: Q21873886 (always incomplete) or Q21873974 (eventually complete) -->|formatter URL\t\t= <!-- for identifiers, URL pattern where $​1 replaces the value -->|external links         = <!-- for identifiers, search string to pass to sister projects' Special:LinkSearch pages, e.g. example.com  -->|see also               = <!-- other, related properties -->|filter\t\t\t= <!-- sample: 7 digit number can be validated with edit filter [[Special:AbuseFilter/17]] -->|robot and gadget jobs\t= <!-- Should or are bots or gadgets doing any task with this? (Checking other properties for consistency, collecting data, etc.) -->|subpage\t\t= Werke ohne Opuszahl|topic\t\t\t= authority control}}====Motivation====This is an important catalogue, which we should be able to reference, possibly as authority control.  I leave it to more experienced folk to consider whether there should be a generic WoO and a separate Kinsky–Halm Catalogue property. (E.G. http://www.raff.org/music/catalog/opus/woo.htm is a WoO catalogue for Joachim Raff.)All&nbsp;the&nbsp;best: ''[[Q23041480|Rich]]&nbsp;[[User talk:Rich Farmbrough|Farmbrough]]'',&nbsp;<small>12:41,&nbsp;4&nbsp;August&nbsp;2019&nbsp;(UTC).</small><br />====\"\n",
    "        # timestamp_row, username_row, anon = extract_info(test, anon)\n",
    "        timestamp_row, username_row, anon = extract_info(df.loc[row,'post'], anon)\n",
    "        save_df=pd.DataFrame({'post':[df.loc[row,'post']],'page_title':[df.loc[row,'page_title']],'thread_title':[df.loc[row,'thread_title']],'urername':[username_row],'timestamp':[timestamp_row]})\n",
    "        save_df.to_csv(str(save_path)+str(dump),mode='a',index=False, header=not os.path.exists(str(save_path)+str(dump)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
