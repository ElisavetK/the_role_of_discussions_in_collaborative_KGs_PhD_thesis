{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example to split item talk pages into threads, posts, words\n",
    "\n",
    "### 1. Split into threads and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import json\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def UTC_separation(thread):\n",
    "    posts_lst=[]\n",
    "    posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',thread)# split the posts based on the timestamp\n",
    "    for i in range(0,len(posts)-1, 2):\n",
    "        posts_lst.append(posts[i]+posts[i+1])# connects post with timestamp because the splot separates them\n",
    "    \n",
    "    return posts_lst\n",
    "\n",
    "#to separate discussions with no title\n",
    "def thread_without_title(temp, page_title, save_path_data):\n",
    "    if ((temp[0:2]=='{{') and (('documentation' in temp) or ('Documentation' in temp))) or ((temp[0:2]=='{{') and (('Interwiki conflict' in temp) or ('interwiki conflict' in temp))) or ('#REDIRECT' in temp): # checks if the text includes only documentation, not discussions\n",
    "         trash=pd.DataFrame({'page_title':[page_title], 'text':temp})\n",
    "         trash.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))\n",
    "    else:\n",
    "        thread_title='No subject'\n",
    "        posts_lst = UTC_separation(temp)\n",
    "        #store posts with the corresponding sibject\n",
    "        df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "        #save to csv \n",
    "        df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "\n",
    "\n",
    "def thread_with_title(titles, temp, page_title, save_path_data):\n",
    "    for j in range(len(temp)):\n",
    "        if (temp[j] in titles):\n",
    "            if (j+1)==(len(temp)):\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = []\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "            else:\n",
    "                thread_title=temp[j]\n",
    "                posts_lst = UTC_separation(temp[j+1])\n",
    "                    #store posts with the corresponding sibject\n",
    "                df=pd.DataFrame({'page_title':[page_title]*len(posts_lst), 'thread_title':[thread_title]*len(posts_lst),'post':posts_lst})\n",
    "                    #save to csv \n",
    "                df.to_csv(str(save_path_data), mode='a',index=False, header=not os.path.exists(str(save_path_data)))\n",
    "        \n",
    "    \n",
    "\n",
    "#function to create a file with the separated posts  \n",
    "def separate_discussions(text, page_title, save_path_data):\n",
    "\n",
    "    titles = re.findall('==(.*)==', text)# find the titles in the discussion  \n",
    "    titles=[s.replace('=', '') for s in titles]# removes unecessery = in the title\n",
    "    \n",
    "    \n",
    "    #titles=list(set(titles)) \n",
    "    temp= text.split('==') # split the text based on == , includided in the title\n",
    "    temp = [x for x in temp if x != ''] # stores the splited parts\n",
    "    temp=[s.strip('=') for s in temp]# removes unecessery = in the title\n",
    "\n",
    "    # ---Step 1: there is no title in the page\n",
    "    if titles==[]: # check if there is title in the begining of the page\n",
    "        thread_without_title(temp[0], page_title, save_path_data) \n",
    "    else:\n",
    "        if temp[0] not in titles:\n",
    "            thread_without_title(temp[0], page_title, save_path_data)# ---Step 2: the first thread do not have title \n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ignores the first if does not have a title and process the rest threads\n",
    "        else:\n",
    "            thread_with_title(titles, temp, page_title, save_path_data)# ---Step 3: a;; threads have titles \n",
    "\n",
    "\n",
    "\n",
    "#this is to extent the size of the reading csv cell\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "file_path='data/all_item_pages/'\n",
    "save_path_trash='trash_files.csv'\n",
    "save_path_data='item_talk_pages.csv' # path to save posts and threads\n",
    "\n",
    "# read the files in a diectory and process\n",
    "for file in os.listdir(str(file_path)):\n",
    "    data=pd.read_csv(str(file_path)+str(file), encoding='utf-8')# read file\n",
    "    for row in range(len(data)):# read row by row (i.e., page by page)\n",
    "        print(row)\n",
    "        page_title=data.loc[row,'page_title']\n",
    "        if isinstance(data.loc[row,'text'],str):# check of the row is nan. If it is nan the type is float not str\n",
    "            separate_discussions(data.loc[row,'text'], page_title, save_path_data)\n",
    "        else:\n",
    "            empty=pd.DataFrame({'page_title':[page_title], 'text':'empty'})\n",
    "            empty.to_csv(str(save_path_trash),mode='a',index=False, header=not os.path.exists(str(save_path_trash)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import preprocess_sentence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# clean and then tokenize the posts\n",
    "def tokenize_post(post):\n",
    "    text=preprocess_sentence(post)\n",
    "    word_list=text.split()\n",
    "    #num_words_text=len(word_list)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def post_level_stats(PATH_input, PATH_output):\n",
    "    data = pd.read_csv(str(PATH_input), header=None, encoding='utf-8')\n",
    "    \n",
    "\n",
    "    new_data = pd.DataFrame(columns=['token_post','num_words', 'thread_subject', 'discussion_page_name'])#create a df to save the info\n",
    "\n",
    "\n",
    "    for row in range(len(data)):\n",
    "        print(row, len(data))\n",
    "        row_post=data.loc[row,'post']\n",
    "        token_post=tokenize_post(row_post)\n",
    "        \n",
    "        save_row={'token_post':token_post,'num_words':len(token_post),'thread_subject':data.loc[row,'thread_subject'],'discussion_page_name':data.loc[row,'discussion_page_name'],'fixed_name':data.loc[row,'fixed_name'],'timestamp':data.loc[row,'timestamp']}\n",
    "        new_data=new_data.append(save_row,ignore_index=True)\n",
    "\n",
    "\n",
    "    new_data.to_csv(str(PATH_output), encoding='utf-8', index=False,  mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_input='item_talk_pages.csv'\n",
    "PATH_output='stats_items.csv'\n",
    "\n",
    "post_level_stats(PATH_input , PATH_output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
