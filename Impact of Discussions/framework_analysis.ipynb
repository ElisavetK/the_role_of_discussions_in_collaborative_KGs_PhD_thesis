{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split the text in case it is very long and produce many tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding\n",
    "\n",
    "openai.api_key = \"---\"\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "#\n",
    "def get_embedding(text, model=embedding_model):\n",
    "   text1=text.split(\" \")\n",
    "   text_len=1.4*len(text1)\n",
    "   if text_len>4000:\n",
    "      num_batches=int(len(text1)//4000)#number of splits\n",
    "      offset=len(text1)//num_batches# length of each split\n",
    "      embedding=np.array(np.zeros(1536),dtype=np.float32)\n",
    "      counter=0\n",
    "      for i in range(0, num_batches+1):\n",
    "         batche_start=i*offset\n",
    "         if batche_start+offset>len(text1):#if we rich the final split \n",
    "            text2=text1[batche_start:]\n",
    "         else:\n",
    "            text2=text1[batche_start:batche_start+offset]\n",
    "        \n",
    "         text3 = \" \".join(text2) \n",
    "         text4=text3.replace(\"\\n\", \" \")\n",
    "         counter +=1\n",
    "         # if batche is empty:\n",
    "         if text4:\n",
    "            # if batche_start==0:\n",
    "            #    embedding=np.array(openai.Embedding.create(input = [text4], model=model)['data'][0]['embedding'],dtype=np.float32)\n",
    "            # else:\n",
    "            print(counter)\n",
    "            # this sum the embeddings\n",
    "            embedding += np.array(openai.Embedding.create(input = [text4], model=model)['data'][0]['embedding'],dtype=np.float32)\n",
    "         else:continue\n",
    "      text5=list(embedding/counter)# after finish with the splits, this is to return the mean embedding\n",
    "      return text5\n",
    "   else:\n",
    "      text = text.replace(\"\\n\", \" \")\n",
    "      return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Drop the specified rows from the DataFrame\n",
    "df = pd.read_csv('items_cleaned.csv',encoding='utf-8')\n",
    "\n",
    "# run all\n",
    "df[\"embedding\"] = df.cleaned_post.apply(lambda x: get_embedding(str(x), model=embedding_model))\n",
    "df.to_csv(\"sav_text_embeddings.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network and create triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "df=pd.read_csv('file_with_item_posts_and_ids.csv')\n",
    "G = nx.Graph()# create cgarph\n",
    "G.add_nodes_from(df['UID'].unique(), type='editor')# add editors\n",
    "G.add_nodes_from(df['TID'].unique(), type='subject')# add subjects\n",
    "print('number of nodes before edges all: ', G.number_of_nodes())\n",
    "\n",
    "for p in range(len(df)):\n",
    "    G.add_node(df.loc[p,'PID'], type='post')\n",
    "    G.add_edge(df.loc[p,'TID'],df.loc[p,'PID'], relation='includes')\n",
    "    G.add_edge(df.loc[p,'UID'],df.loc[p,'PID'], relation='said')\n",
    "    \n",
    "\n",
    "print('number of nodes: ', G.number_of_nodes())\n",
    "print('number of edges : ', G.number_of_edges())\n",
    "\n",
    "# save triples\n",
    "f = open('save_triples.txt', 'w')#open a txt\n",
    "for line in nx.generate_edgelist(G, data=['relation']):\n",
    "    triple = line.split(' ') # change the order in the last two elements\n",
    "    # triple[-2], triple[-1] = triple[-1], triple[-2]\n",
    "    username=' '.join(triple[:-2])\n",
    "    row=str(username)+'\\t'+str(triple[-1])+'\\t'+str(triple[-2])+'\\n'\n",
    "    print(row)\n",
    "    f.write(row)\n",
    "\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykeen\n",
    "pykeen.env()\n",
    "\n",
    "#######------------ load data and split to train test\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"triples.txt\", encoding='utf-8', delimiter='\\t', header=None).astype(\"str\")\n",
    "df.columns=['subject', 'predicate', 'object']\n",
    "\n",
    "\n",
    "#split train and test set\n",
    "from pykeen.triples import TriplesFactory\n",
    "# Assuming your dataframe has columns 'subject', 'predicate', 'object'\n",
    "triples_factory = TriplesFactory.from_labeled_triples(\n",
    "    triples=df[['subject', 'predicate', 'object']].values , create_inverse_triples=True\n",
    ")\n",
    "\n",
    "training = triples_factory\n",
    "validation = triples_factory\n",
    "testing = triples_factory\n",
    "\n",
    "d=training\n",
    "id_to_entity={v: k for k, v in d.entity_to_id.items()}\n",
    "id_to_relation={v: k for k, v in d.relation_to_id.items()}\n",
    "\n",
    "# Display the first few triples\n",
    "triples_factory.triples\n",
    "\n",
    "\n",
    "###-------------Run --------\n",
    "# run model\n",
    "from pykeen.pipeline import pipeline\n",
    "import torch\n",
    "\n",
    "result = pipeline(\n",
    "    # model='CompGCN',\n",
    "    model='CompGCN',\n",
    "    loss=\"softplus\",\n",
    "    training=training,\n",
    "    testing=testing,\n",
    "    validation=validation,\n",
    "    model_kwargs=dict(embedding_dim=100),  # Increase the embedding dimension\n",
    "    optimizer_kwargs=dict(lr=0.1),  # Adjust the learning rate\n",
    "    training_kwargs=dict(num_epochs=100, use_tqdm_batch=False),  # Increase the number of epochs\n",
    ")\n",
    "\n",
    "# The trained model is stored in the pipeline result\n",
    "model = result.model\n",
    "\n",
    "# save the model\n",
    "torch.save(model,'model.pkl')\n",
    "my_pykeen_model = torch.load('model.pkl')\n",
    "\n",
    "\n",
    "###3----------------print stats froevaluation\n",
    "\n",
    "# evaluate\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "\n",
    "# Create an evaluator\n",
    "evaluator = RankBasedEvaluator()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluator.evaluate(my_pykeen_model, testing.mapped_triples, additional_filter_triples=[training.mapped_triples, validation.mapped_triples])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Hits@1: {metrics.get_metric('hits@1')}\")\n",
    "print(f\"Hits@3: {metrics.get_metric('hits@3')}\")\n",
    "print(f\"Hits@5: {metrics.get_metric('hits@5')}\")\n",
    "print(f\"Hits@10: {metrics.get_metric('hits@10')}\")\n",
    "print(f\"Mean Reciprocal Rank: {metrics.get_metric('mean_reciprocal_rank')}\")\n",
    "\n",
    "\n",
    "\n",
    "### -------------- save embeddings------------\n",
    "\n",
    "a=my_pykeen_model.entity_representations\n",
    "b=a[0].combined.entity_representations._embeddings.weight # this is for CompGCN\n",
    "# b=a[0]._embeddings.weight # this is for TransR\n",
    "p=triples_factory.entity_to_id\n",
    "r=triples_factory.relation_to_id\n",
    "\n",
    "embedding_dict={}\n",
    "for entity in p.keys():\n",
    "  id=p[entity]\n",
    "  embedding=b[id]\n",
    "  embedding_dict[entity]=embedding.cpu().detach().numpy()\n",
    "\n",
    "df_embeddings_ids = pd.DataFrame.from_dict(embedding_dict, orient='index')\n",
    "\n",
    "df_embeddings_ids.to_csv('save_graph_embeddings.csv',encoding='utf-8',header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#load data after creating one file including all features for prediction\n",
    "\n",
    "#Posts\n",
    "# data = pd.read_csv('training_thread.csv',encoding='utf-8')\n",
    "data = pd.read_csv('training_individual_posts.csv',encoding='utf-8')\n",
    "# data = pd.read_csv('training_first_posts.csv',encoding='utf-8')\n",
    "\n",
    "#Editors\n",
    "# data = pd.read_csv('/training_editors.csv',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "from keras.callbacks import EarlyStopping \n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_keras(concatenated_arr, status, n):\n",
    "\n",
    "    print('============= n= ', str(n), ' ================')\n",
    "    print('=============================================')\n",
    "    print('\\n')\n",
    "    # Split the data into train, validation, and test sets\n",
    "    # First, split into train and temp sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        concatenated_arr,\n",
    "        status,\n",
    "        test_size=0.4,  # 60% for training and temp\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Then, split the temp set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        test_size=0.5,  # 50% of the temp set for validation and test each\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience=20)\n",
    "\n",
    "    # Define the input layers\n",
    "    input = Input(shape=(X_test.shape[1],))\n",
    "\n",
    "    if n==1.8:\n",
    "        # Define the neural network layers\n",
    "        x = Dense(8, activation='relu')(input)\n",
    "    elif n==1.32:\n",
    "        x = Dense(32, activation='relu')(input)\n",
    "    elif n==1.64:\n",
    "        x = Dense(64, activation='relu')(input)\n",
    "    elif n==2:\n",
    "        x = Dense(8, activation='relu')(input)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "    elif n==2.1:\n",
    "        x = Dense(32, activation='relu')(input)\n",
    "        x = Dense(364, activation='relu')(x)\n",
    "    elif n==3:\n",
    "        x = Dense(8, activation='relu')(input)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                    classes=np.unique(y_train),\n",
    "                                                    y=y_train)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train,\n",
    "            y_train,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(X_val, y_val),\n",
    "            class_weight=dict(zip(np.unique(y_train),class_weights)),\n",
    "            callbacks=[es]\n",
    "            )\n",
    "\n",
    "\n",
    "    print(Counter(y_test))\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    y_pred_class = np.round(predictions)  # Predicted class labels\n",
    "    # Calculate evaluation metrics\n",
    "    report = classification_report(y_test, y_pred_class)\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_test, y_pred_class, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_class, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
    "    acc = accuracy_score(y_test, y_pred_class)\n",
    "    cm = confusion_matrix(y_test, y_pred_class)\n",
    "    # report = classification_report( y_test,y_pred_tree)\n",
    "\n",
    "    #Print the precision, recall, and F1-score\n",
    "    print(\"Precision tree:\", precision)\n",
    "    print(\"Recall tree:\", recall)\n",
    "    print(\"F1 Score tree:\", f1)\n",
    "    print(\"Acc score tree:\", acc)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(print(cm))\n",
    "\n",
    "\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove negative age\n",
    "data=data[data['account_age']>=0]\n",
    "\n",
    "# convert embeddings from stings to lists\n",
    "data['graph']=data['graph'].apply(lambda x: np.array(ast.literal_eval(x), dtype='float32'))\n",
    "data['text']=data['text'].apply(lambda x: np.array(ast.literal_eval(x), dtype='float32'))\n",
    "\n",
    "#convert data to arrays\n",
    "t_embeddings=np.vstack(data['text'].values)\n",
    "g_embeddings=np.vstack(data['graph'].values)\n",
    "labels=np.array(data['answer2post'])\n",
    "age=np.array(data['account_age'])\n",
    "edits=np.array(data['num_edits'])\n",
    "posts=np.array(data['num_posts'])\n",
    "status=np.array(data['status'])\n",
    "rights=np.array(data['rights'])\n",
    "\n",
    "\n",
    "#Normalise the continious values\n",
    "scaler = StandardScaler()\n",
    "age_norm=scaler.fit_transform(age.reshape(-1, 1)).flatten()\n",
    "edits_norm=scaler.fit_transform(edits.reshape(-1, 1)).flatten()\n",
    "posts_norm=scaler.fit_transform(posts.reshape(-1, 1)).flatten()\n",
    "\n",
    "#encode the categorical values\n",
    "label_encoder = LabelEncoder()\n",
    "status_enc=label_encoder.fit_transform(status)\n",
    "# label_enc=label_encoder.fit_transform(labels)\n",
    "rights_enc=label_encoder.fit_transform(rights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatinate data and run the different cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run features\n",
    "file='G_Tsum'\n",
    "# concatenated_arr=np.vstack(age_norm)\n",
    "# concatenated_arr=np.vstack(edits_norm)\n",
    "# concatenated_arr=np.vstack(posts_norm)\n",
    "# concatenated_arr=np.vstack(rights_enc)\n",
    "# concatenated_arr=np.vstack(status_enc)\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,status_enc,rights_enc )).T\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,rights_enc )).T\n",
    "\n",
    "# # # run graph\n",
    "# concatenated_arr = g_embeddings\n",
    "\n",
    "#run text\n",
    "# concatenated_arr = t_embeddings\n",
    "\n",
    "\n",
    "#run features and graph\n",
    "# # concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,status_enc,rights_enc)).T\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,rights_enc )).T\n",
    "# concatenated_arr=np.concatenate((concatenated_arr,g_embeddings),axis=1)\n",
    "\n",
    "\n",
    "# # #run features and text\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,status_enc,rights_enc)).T\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,rights_enc )).T\n",
    "# concatenated_arr=np.concatenate((concatenated_arr,t_embeddings),axis=1)\n",
    "\n",
    "# #run graph and text\n",
    "concatenated_arr = np.concatenate((g_embeddings,t_embeddings),axis=1)\n",
    "\n",
    "\n",
    "# # #run all\n",
    "# # concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,status_enc,rights_enc)).T\n",
    "# concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,rights_enc )).T\n",
    "# concatenated_arr=np.concatenate((concatenated_arr,g_embeddings,t_embeddings),axis=1)\n",
    "\n",
    "# # #run age and text\n",
    "# # concatenated_arr = np.vstack((age_norm, edits_norm,posts_norm,status_enc,rights_enc)).T\n",
    "# concatenated_arr = np.vstack(age_norm)\n",
    "# concatenated_arr=np.concatenate((concatenated_arr,t_embeddings),axis=1)\n",
    "\n",
    "import sys \n",
    "stdoutOrigin=sys.stdout \n",
    "sys.stdout = open(\"log_output_posts_\"+str(file)+\".txt\", \"w\")\n",
    "\n",
    "dense_list=[1.8, 1.32, 1.64, 2, 2.1, 3]\n",
    "\n",
    "for n in dense_list:\n",
    "    run_keras(concatenated_arr,labels, n)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
